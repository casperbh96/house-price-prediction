{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREVIOUS WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from scipy.stats import skew\n",
    " \n",
    "def mean_squared_error_(ground_truth, predictions):\n",
    "    return mean_squared_error(ground_truth, predictions) ** 0.5\n",
    "RMSE = make_scorer(mean_squared_error_, greater_is_better=False)    \n",
    "    \n",
    "def create_submission(prediction,score):\n",
    "    now = datetime.datetime.now()\n",
    "    sub_file = 'submission_'+str(score)+'_'+str(now.strftime(\"%Y-%m-%d-%H-%M\"))+'.csv'\n",
    "    print ('Creating submission: ', sub_file)\n",
    "    pd.DataFrame({'Id': test['Id'].values, 'SalePrice': prediction}).to_csv(sub_file, index=False)\n",
    "\n",
    "def data_preprocess(train,test):\n",
    "    outlier_idx = [4,11,13,20,46,66,70,167,178,185,199, 224,261, 309,313,318, 349,412,423,440,454,477,478, 523,540, 581,588,595,654,688, 691, 774, 798, 875, 898,926,970,987,1027,1109, 1169,1182,1239, 1256,1298,1324,1353,1359,1405,1442,1447]\n",
    "    train.drop(train.index[outlier_idx],inplace=True)\n",
    "    all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n",
    "                          test.loc[:,'MSSubClass':'SaleCondition']))\n",
    "    \n",
    "    to_delete = ['Alley','FireplaceQu','PoolQC','Fence','MiscFeature']\n",
    "    all_data = all_data.drop(to_delete,axis=1)\n",
    "\n",
    "    train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "    #log transform skewed numeric features\n",
    "    numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "    skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "    skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "    skewed_feats = skewed_feats.index\n",
    "    all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "    all_data = pd.get_dummies(all_data)\n",
    "    all_data = all_data.fillna(all_data.mean())\n",
    "    X_train = all_data[:train.shape[0]]\n",
    "    X_test = all_data[train.shape[0]:]\n",
    "    y = train.SalePrice\n",
    "\n",
    "    return X_train,X_test,y\n",
    "    \n",
    "def model_random_forecast(Xtrain,Xtest,ytrain):\n",
    "    \n",
    "    X_train = Xtrain\n",
    "    y_train = ytrain\n",
    "    rfr = RandomForestRegressor(n_jobs=1, random_state=0)\n",
    "    param_grid = {'n_estimators': [1000]}\n",
    "    # 'n_estimators': [1000], 'max_features': [10,15,20,25], 'max_depth':[20,20,25,25,]}\n",
    "    model = GridSearchCV(estimator=rfr, param_grid=param_grid, n_jobs=1, cv=10, scoring=RMSE)\n",
    "    model.fit(X_train, y_train)\n",
    "    print('Random forecast regression...')\n",
    "    print('Best Params:')\n",
    "    print(model.best_params_)\n",
    "    print('Best CV Score:')\n",
    "    print(-model.best_score_)\n",
    "\n",
    "    y_pred = model.predict(Xtest)\n",
    "    return y_pred, -model.best_score_\n",
    "\n",
    "\n",
    "def model_extra_trees_regression(Xtrain,Xtest,ytrain):\n",
    "    \n",
    "    X_train = Xtrain\n",
    "    y_train = ytrain\n",
    "    \n",
    "    etr = ExtraTreesRegressor(n_jobs=1, random_state=0)\n",
    "    param_grid = {}#'n_estimators': [500], 'max_features': [10,15,20]}\n",
    "    model = GridSearchCV(estimator=etr, param_grid=param_grid, n_jobs=1, cv=10, scoring=RMSE)\n",
    "    model.fit(X_train, y_train)\n",
    "    print('Extra trees regression...')\n",
    "    print('Best Params:')\n",
    "    print(model.best_params_)\n",
    "    print('Best CV Score:')\n",
    "    print(-model.best_score_)\n",
    "\n",
    "    y_pred = model.predict(Xtest)\n",
    "    return y_pred, -model.best_score_\n",
    "\n",
    "\n",
    "# read data, build model and do prediction\n",
    "# read train data\n",
    "train = pd.read_csv(\"../../input/train.csv\")\n",
    "# read test data \n",
    "test = pd.read_csv(\"../../input/test.csv\") \n",
    "Xtrain,Xtest,ytrain = data_preprocess(train,test)\n",
    "\n",
    "\n",
    "test_predict,score = model_random_forecast(Xtrain,Xtest,ytrain)\n",
    "# test_predict,score = model_extra_trees_regression(Xtrain,Xtest,ytrain)\n",
    "\n",
    "create_submission(np.exp(test_predict),score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Python Imports\n",
    "import math, time, random, datetime\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "import missingno\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Preprocessing\n",
    "from scipy.linalg import svd\n",
    "import sklearn\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Modelling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "sample_submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_or_nan_values(df):\n",
    "    fill_with = 0\n",
    "    \n",
    "    # Get the most common element by using size(),\n",
    "    # which returns the element and how common it is\n",
    "    for column in df:\n",
    "        \n",
    "        # Check if the column is an object, float64 or int64\n",
    "        is_it_float = (df[column].dtype == np.float64)\n",
    "        is_it_int = (df[column].dtype == np.int64)\n",
    "        \n",
    "        # If it is an object,\n",
    "        # find the most common element and fill missing and NaN values\n",
    "        if(not is_it_float and not is_it_int):\n",
    "            fill_with = df[column].mode().item()\n",
    "                    \n",
    "        # If it is either a float64 or int64,\n",
    "        # then calculate the mean and fill missing and NaN values\n",
    "        else:\n",
    "            if is_it_float:\n",
    "                fill_with = np.nanmean(df[column], dtype=np.float64)\n",
    "            if is_it_int:\n",
    "                fill_with = np.nanmean(df[column], dtype=np.int64)\n",
    "        \n",
    "        # Fill the values in our dataset\n",
    "        df[column] = df[column].fillna(fill_with)\n",
    "        fill_with = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caspe\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Concatenate all of data\n",
    "cc_data = pd.concat([train, test])\n",
    "cc_data = cc_data.drop(['Id', 'SalePrice','Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis=1)\n",
    "\n",
    "# Get the SalePrice as the natural logarithm\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "y = train['SalePrice']\n",
    "\n",
    "# One-Hot encode all data\n",
    "cc_data = pd.get_dummies(cc_data, prefix_sep='_')\n",
    "\n",
    "# Fill missing or NaN values\n",
    "fill_missing_or_nan_values(cc_data)\n",
    "\n",
    "# Slice data, train.shape[0] is the observations\n",
    "# 1) from start to middle of observations\n",
    "# 2) from middle of observations to end\n",
    "X_train = cc_data[:train.shape[0]]\n",
    "X_test = cc_data[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is dataframe, y is output, m is how many features you want selected\n",
    "# returns array of highest scoring features\n",
    "def feature_selection(X, y, m):\n",
    "    # Data is standardized here, minus mean and divided by standard deviation\n",
    "    # The correlation between each regressor and the target is computed\n",
    "    # It is converted to an F score then to a p-value, which is returned\n",
    "    f_regression = lambda X,y : sklearn.feature_selection.f_regression(X,y,center=False)\n",
    "\n",
    "    # removes all but the  highest scoring features\n",
    "    featureSelector = SelectKBest(score_func=f_regression,k=m)\n",
    "    featureSelector.fit(X,y)\n",
    "    high_score_arr = [X.columns[1+zero_based_index] for zero_based_index in list(featureSelector.get_support(indices=True))]\n",
    "    \n",
    "    return high_score_arr\n",
    "\n",
    "selected_features = feature_selection(df_train,y,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the best scores from each k-fold\n",
    "def random_forest_model(X, y):\n",
    "    # Perform Grid-Search to find optimal parameters\n",
    "    # Use RandomForestRegressor as the estimator,\n",
    "    # Find optimal max_depth and n_estimators\n",
    "    gsc = GridSearchCV(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        param_grid={\n",
    "            'max_depth': [3, None],\n",
    "            'n_estimators': (10, 20, 30, 40, 50, 100, 300, 500, 1000),\n",
    "        },\n",
    "        cv=5, scoring='neg_mean_squared_log_error', verbose=0, n_jobs=-1)\n",
    "    \n",
    "    # Actually run the model and get the best params\n",
    "    model = gsc.fit(X, y)\n",
    "    params = model.best_params_\n",
    "    \n",
    "    # Take the best params from Grid-Search and use here\n",
    "    rfr = RandomForestRegressor(max_depth=params[\"max_depth\"], n_estimators=params[\"n_estimators\"],\n",
    "                                random_state=False, verbose=False)\n",
    "    \n",
    "    # Perform K-Fold CV, to find RMSLE\n",
    "    RMSLE_scores = cross_val_score(rfr, X, y, cv=10, n_jobs=-1, scoring=\"neg_mean_squared_log_error\")\n",
    "    \n",
    "    return RMSLE_scores\n",
    "\n",
    "scores = random_forest_model(df_train[selected_features],y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE: 0.014 (0.003)\n"
     ]
    }
   ],
   "source": [
    "# Root Mean Squared Logarithmic Error (RMSLE)\n",
    "print(\"RMSLE: %.3f (%.3f)\" % (scores.mean()*100.0*-1, scores.std()*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_model1(_Xtrain,_Xtest,y):\n",
    "    gs = GridSearchCV(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        param_grid={\n",
    "            'max_depth': [3, None],\n",
    "            'n_estimators': (10, 30, 50)\n",
    "        }, cv=10, n_jobs=-1, scoring='neg_mean_squared_log_error'\n",
    "    )\n",
    "    model = gs.fit(_Xtrain,y)\n",
    "    pred = model.predict(_Xtest)\n",
    "    \n",
    "    # return all predictions and mean of all cross validated scores\n",
    "    return pred, -model.best_score_\n",
    "\n",
    "y_pred,score = random_forest_model1(X_train, X_test, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[126008.52041343 153244.62924091 183281.40511193 ... 157776.07019133\n",
      " 111311.35136438 233282.04387426] 0.00012299529870924122\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(y_pred),score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# s can be array of functions for different models to run\n",
    "def kfold_cv_models(k,s):\n",
    "    best_models_test_error = 0\n",
    "\n",
    "    for fold in range(k):\n",
    "        # we have k partitions\n",
    "        # let df_train and df_test be the next partition\n",
    "        for model in range(s):\n",
    "            # run next model with current k with df_train\n",
    "            # get test error of df_test\n",
    "            if this_models_test_error < best_models_test_error:\n",
    "                best_models_test_error = this_models_test_error\n",
    "    return best_models_test_error\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 17622.44 degrees.\n"
     ]
    }
   ],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.19 %.\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / test_labels)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(978, 50)\n",
      "(482, 50)\n",
      "Accuracy: 88.473%\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.33\n",
    "seed = 7\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df_train[selected_features], y, test_size=test_size, random_state=seed)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "model = LinearRegression() \n",
    "model.fit(X_train, Y_train)\n",
    "result = model.score(X_test, Y_test)\n",
    "print(\"Accuracy: %.3f%%\" % (result*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.014% (0.007%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(model, df_train, y, cv=kfold, scoring=\"neg_mean_squared_log_error\")\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0*-1, results.std()*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 1.30726717e-03 1.09002673e-02 8.22600933e-01\n",
      " 0.00000000e+00 2.24175335e-04 3.38865375e-04 3.14993144e-03\n",
      " 0.00000000e+00 1.27771514e-02 2.40628056e-02 4.82529222e-02\n",
      " 0.00000000e+00 1.97217104e-04 0.00000000e+00 7.64012279e-03\n",
      " 1.06137293e-02 2.36425450e-04 4.35963273e-02 1.13380047e-02\n",
      " 1.55509357e-03 2.18581446e-04 0.00000000e+00 2.15142708e-04\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 5.07273835e-05 7.89680401e-05 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 4.47795587e-04 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 1.97546301e-04]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Utilities_NoSeWa' 'RoofMatl_Membran' 'Heating_Floor'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-fedc4b9bf547>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mselected_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2680\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2681\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2682\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2683\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2684\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2724\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2725\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2726\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2727\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[0;32m   1325\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[1;32m-> 1327\u001b[1;33m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[0;32m   1328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Utilities_NoSeWa' 'RoofMatl_Membran' 'Heating_Floor'] not in index\""
     ]
    }
   ],
   "source": [
    "regr = RandomForestRegressor(max_depth=3, random_state=0, n_estimators=100)\n",
    "regr.fit(train[selected_features], y)\n",
    "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
    "           max_features='auto', max_leaf_nodes=None,\n",
    "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "           min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "           oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
    "\n",
    "print(regr.feature_importances_)\n",
    "\n",
    "print(regr.predict(test[selected_features]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
