{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREVIOUS WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Python Imports\n",
    "import math, time, random, datetime\n",
    "from math import sqrt\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "import missingno\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Preprocessing\n",
    "from scipy.linalg import svd\n",
    "import sklearn\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from fancyimpute import IterativeImputer\n",
    "from scipy.stats import zscore\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Modelling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV, KFold, ShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "sample_submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_or_nan_values(df):\n",
    "    fill_with = 0\n",
    "    \n",
    "    # Get the most common element by using size(),\n",
    "    # which returns the element and how common it is\n",
    "    for column in df:\n",
    "        \n",
    "        # Check if the column is an object, float64 or int64\n",
    "        is_it_float = (df[column].dtype == np.float64)\n",
    "        is_it_int = (df[column].dtype == np.int64)\n",
    "        \n",
    "        # If it is an object,\n",
    "        # find the most common element and fill missing and NaN values\n",
    "        if(not is_it_float and not is_it_int):\n",
    "            fill_with = df[column].mode().item()\n",
    "                    \n",
    "        # If it is either a float64 or int64,\n",
    "        # then calculate the mean and fill missing and NaN values\n",
    "        else:\n",
    "            if is_it_float:\n",
    "                fill_with = np.nanmean(df[column], dtype=np.float64)\n",
    "            if is_it_int:\n",
    "                fill_with = np.nanmean(df[column], dtype=np.int64)\n",
    "        \n",
    "        # Fill the values in our dataset\n",
    "        df[column] = df[column].fillna(fill_with)\n",
    "        fill_with = 0\n",
    "\n",
    "def fill_ii(df):\n",
    "    df_filled_ii = pd.DataFrame(IterativeImputer().fit_transform(df.values))\n",
    "    df_filled_ii.columns = df.columns\n",
    "    df_filled_ii.index = df.index\n",
    "\n",
    "    return df_filled_ii\n",
    "\n",
    "def pca(df):\n",
    "    pca = PCA(.95)\n",
    "    df = pca.fit_transform(df)\n",
    "    return df\n",
    "\n",
    "# This function removes all observations that are more than\n",
    "# three standard deviations away from the mean\n",
    "def remove_outliers(df):\n",
    "    '''\n",
    "    numeric_features = train.select_dtypes(include=[np.number])\n",
    "    print(len(numeric_features.columns))\n",
    "    print(numeric_features.columns)\n",
    "    fig, axes = plt.subplots(ncols=5, nrows=8, figsize=(16, 40))\n",
    "    axes = np.ravel(axes)\n",
    "\n",
    "    col_name = ['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual',\n",
    "           'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n",
    "           'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n",
    "           'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
    "           'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n",
    "           'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n",
    "           'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n",
    "           'MiscVal', 'MoSold', 'YrSold', 'SalePrice']\n",
    "    for i, c in zip(range(38), col_name):\n",
    "        train.plot.scatter(ax=axes[i], x=c, y='SalePrice', sharey=True, colorbar=False, c='r')\n",
    "    '''\n",
    "    df = df.drop(df[(df['LotArea']>100000)].index)\n",
    "    df = df.drop(df[(df['BsmtFinSF1']>4000)].index)\n",
    "    df = df.drop(df[(df['TotalBsmtSF']>4000)].index)\n",
    "    df = df.drop(df[(df['1stFlrSF']>4000)].index)\n",
    "    df = df.drop(df[(df['GrLivArea']>4000)].index)\n",
    "    \n",
    "    return df\n",
    "        \n",
    "def data_engineering(train, test):\n",
    "    # Make train and test equal have the same shape\n",
    "    train = train.drop(train.index[0])\n",
    "    \n",
    "    # Concatenate all of data\n",
    "    cc_data = pd.concat([train, test], sort=False)\n",
    "    cc_data = cc_data.drop(['Id', 'SalePrice','Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis=1)\n",
    "    \n",
    "    # Get the SalePrice as the natural logarithm\n",
    "    train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "    y = train['SalePrice']\n",
    "    \n",
    "    # Remove outliers from data\n",
    "    #cc_data = remove_outliers(cc_data)\n",
    "    \n",
    "    # One-Hot encode all data\n",
    "    cc_data = pd.get_dummies(cc_data, prefix_sep='_')\n",
    "    \n",
    "    # Impute all data, using IterativeImputer\n",
    "    cc_data = fill_ii(cc_data)\n",
    "    #return cc_data,y\n",
    "    # Slice data, train.shape[0] is the observations\n",
    "    # 1) from start to middle of observations\n",
    "    # 2) from middle of observations to end\n",
    "    X_train = cc_data[:train.shape[0]]\n",
    "    X_test = cc_data[train.shape[0]:]\n",
    "    \n",
    "    #X_train = X_train.drop(X_train.index[0])\n",
    "    #y = y.drop(y.index[0])\n",
    "    \n",
    "    return X_train,X_test,y\n",
    "\n",
    "# X is dataframe, y is output, m is how many features you want selected\n",
    "# returns array of highest scoring features\n",
    "def feature_selection(X, y, m):\n",
    "    # Data is standardized here, minus mean and divided by standard deviation\n",
    "    # The correlation between each regressor and the target is computed\n",
    "    # It is converted to an F score then to a p-value, which is returned\n",
    "    f_regression = lambda X,y : sklearn.feature_selection.f_regression(X,y,center=False)\n",
    "\n",
    "    # removes all but the  highest scoring features\n",
    "    featureSelector = SelectKBest(score_func=f_regression,k=m)\n",
    "    featureSelector.fit(X,y)\n",
    "    high_score_arr = [X.columns[1+zero_based_index] for zero_based_index in list(featureSelector.get_support(indices=True))]\n",
    "    \n",
    "    return high_score_arr\n",
    "\n",
    "df_train,df_test,y = data_engineering(train,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crossvalidation outer fold: 1/10\n",
      "X_train_outer 146\n",
      "y_train_outer 146\n",
      "X_test_outer 1313\n",
      "y_test_outer 1313\n",
      "Optimal parameters for algorithm:\n",
      "{'max_depth': [3], 'max_features': [5], 'n_estimators': [20]}\n",
      "\n",
      "Crossvalidation outer fold: 2/10\n",
      "X_train_outer 146\n",
      "y_train_outer 146\n",
      "X_test_outer 1313\n",
      "y_test_outer 1313\n",
      "Optimal parameters for algorithm:\n",
      "{'max_depth': [3], 'max_features': [5], 'n_estimators': [20]}\n",
      "\n",
      "Crossvalidation outer fold: 3/10\n",
      "X_train_outer 146\n",
      "y_train_outer 146\n",
      "X_test_outer 1313\n",
      "y_test_outer 1313\n",
      "Optimal parameters for algorithm:\n",
      "{'max_depth': [3], 'max_features': [5], 'n_estimators': [20]}\n",
      "\n",
      "Crossvalidation outer fold: 4/10\n",
      "X_train_outer 146\n",
      "y_train_outer 146\n",
      "X_test_outer 1313\n",
      "y_test_outer 1313\n",
      "Optimal parameters for algorithm:\n",
      "{'max_depth': [3], 'max_features': [5], 'n_estimators': [20]}\n",
      "\n",
      "Crossvalidation outer fold: 5/10\n",
      "X_train_outer 146\n",
      "y_train_outer 146\n",
      "X_test_outer 1313\n",
      "y_test_outer 1313\n",
      "Optimal parameters for algorithm:\n",
      "{'max_depth': [3], 'max_features': [5], 'n_estimators': [20]}\n",
      "\n",
      "Crossvalidation outer fold: 6/10\n",
      "X_train_outer 146\n",
      "y_train_outer 146\n",
      "X_test_outer 1313\n",
      "y_test_outer 1313\n",
      "Optimal parameters for algorithm:\n",
      "{'max_depth': [3], 'max_features': [5], 'n_estimators': [20]}\n",
      "\n",
      "Crossvalidation outer fold: 7/10\n",
      "X_train_outer 146\n",
      "y_train_outer 146\n",
      "X_test_outer 1313\n",
      "y_test_outer 1313\n",
      "Optimal parameters for algorithm:\n",
      "{'max_depth': [3], 'max_features': [5], 'n_estimators': [20]}\n",
      "\n",
      "Crossvalidation outer fold: 8/10\n",
      "X_train_outer 146\n",
      "y_train_outer 146\n",
      "X_test_outer 1313\n",
      "y_test_outer 1313\n",
      "Optimal parameters for algorithm:\n",
      "{'max_depth': [3], 'max_features': [5], 'n_estimators': [20]}\n",
      "\n",
      "Crossvalidation outer fold: 9/10\n",
      "X_train_outer 146\n",
      "y_train_outer 146\n",
      "X_test_outer 1313\n",
      "y_test_outer 1313\n",
      "Optimal parameters for algorithm:\n",
      "{'max_depth': [3], 'max_features': [5], 'n_estimators': [20]}\n",
      "\n",
      "Crossvalidation outer fold: 10/10\n",
      "X_train_outer 145\n",
      "y_train_outer 145\n",
      "X_test_outer 1314\n",
      "y_test_outer 1314\n",
      "Optimal parameters for algorithm:\n",
      "{'max_depth': [3], 'max_features': [5], 'n_estimators': [20]}\n",
      "[0.04840561 0.04623955 0.04690715 0.06187937 0.05493093 0.04304262\n",
      " 0.06101604 0.06030157 0.05284923 0.1131892 ]\n",
      "Generalization Error with RMSLE as scoring: 0.24264403050682473\n"
     ]
    }
   ],
   "source": [
    "opt_params = []\n",
    "mse_validation = []\n",
    "dic_values_to_array = lambda dic: {key: [dic[key]]  for key in dic}\n",
    "def random_forest_validation(X_train,y_train,X_test,params_grid,validate = True):\n",
    "    gs = GridSearchCV(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        param_grid=params_grid, cv=ShuffleSplit(test_size=0.10, n_splits =1, random_state=0),\n",
    "        n_jobs=-1, scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    model = gs.fit(X_train,y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    score = -model.best_score_\n",
    "    \n",
    "    if validate:\n",
    "        opt_params.append(dic_values_to_array(model.best_params_))\n",
    "        mse_validation.append(score)\n",
    "        return mse_validation,opt_params\n",
    "    else:\n",
    "        return pred,score\n",
    "\n",
    "def nested_cv(train_data,test_data,y,outer,inner):\n",
    "    '''\n",
    "    This function runs nested cross-validation, where the optimal parameters\n",
    "    for a random forest algorithm is searched for in the inner loop and\n",
    "    applied in the outer loop.\n",
    "    \n",
    "    train_data: your training data\n",
    "    test_data: your testing data\n",
    "    y: your output variable\n",
    "    outer: how many k-folds we split the data in the outer loop\n",
    "    inner: how many k-folds we split the data in the inner loop\n",
    "    '''\n",
    "    CV = KFold(outer, shuffle=True)\n",
    "    generalization_error = np.zeros(outer)\n",
    "    #\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "    #\n",
    "    for (i, (train_index, test_index)) in enumerate(CV.split(train_data,test_data,y)):\n",
    "        print('\\nCrossvalidation outer fold: {0}/{1}'.format(i+1,outer))\n",
    "        X_train_outer = train_data[train_index.shape[0]:]\n",
    "        y_train_outer = y[train_index.shape[0]:]\n",
    "        X_test_outer = test_data[test_index.shape[0]:]\n",
    "        y_test_outer = y[test_index.shape[0]:]\n",
    "        print(\"X_train_outer\",X_train_outer.shape[0])\n",
    "        print(\"y_train_outer\",y_train_outer.shape[0])\n",
    "        print(\"X_test_outer\",X_test_outer.shape[0])\n",
    "        print(\"y_test_outer\",y_test_outer.shape[0])\n",
    "        \n",
    "        params_grid={\n",
    "                'max_depth': [3, None],\n",
    "                'n_estimators': (10, 20),# 30, 50, 100, 200, 400, 600, 800, 1000),\n",
    "                'max_features': (5,10,15,20)\n",
    "        }\n",
    "        \n",
    "        opt_params = []\n",
    "        mse_validation = []\n",
    "        for (j, (train_index_inner, test_index_inner)) in enumerate(CV.split(X_train_outer,y_train_outer)):\n",
    "            X_train_inner = X_train_outer[train_index_inner.shape[0]:]\n",
    "            y_train_inner = y_train_outer[train_index_inner.shape[0]:]\n",
    "            X_test_inner = X_train_outer[test_index_inner.shape[0]:]\n",
    "            y_test_inner = y_train_outer[test_index_inner.shape[0]:]\n",
    "            mse_validation,opt_params = random_forest_validation(X_train_inner,y_train_inner,X_test_inner,params_grid)\n",
    "        \n",
    "        lowest_mse = 999999999\n",
    "        for idx,mse in enumerate(mse_validation):\n",
    "            if mse < lowest_mse:\n",
    "                lowest_mse = mse\n",
    "                params_grid = opt_params[idx]\n",
    "        pred, score = random_forest_validation(X_train_outer,y_train_outer,X_test_outer,params_grid, validate=False)\n",
    "        generalization_error[i] = score\n",
    "        print(\"Optimal parameters for algorithm:\\n{0}\".format(params_grid))\n",
    "    print(generalization_error)\n",
    "    print(\"Generalization Error with RMSLE as scoring: {0}\".format(sqrt(np.mean(generalization_error))))\n",
    "nested_cv(df_train,df_test,y,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_prediction(X_train,X_test,y_real):\n",
    "    gs = GridSearchCV(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        param_grid={\n",
    "            'max_depth': [3, None],\n",
    "            'n_estimators': (10, 30, 50, 100, 200, 400, 600, 800, 1000),\n",
    "            'max_features': (2,4,6)\n",
    "        }, cv=10, n_jobs=-1, scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    model = gs.fit(X_train,y_real)\n",
    "    pred = model.predict(X_test)\n",
    "    score = sqrt(-model.best_score_)\n",
    "    \n",
    "    # return all predictions and mean of all cross validated scores\n",
    "    return pred, score, model\n",
    "\n",
    "df_train,df_test,y = data_engineering(train,test)\n",
    "#selected_features = feature_selection(df_train, y, 50)\n",
    "\n",
    "#pred,score, model = random_forest_prediction(df_train, df_test, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=6, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=200, n_jobs=None, oob_score=False,\n",
      "           random_state=None, verbose=0, warm_start=False)\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "print(model.best_estimator_)\n",
    "print(model.best_estimator_.n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.76359381 11.96945027 12.1351863  ... 11.96083966 11.71628178\n",
      " 12.28194491]\n",
      "0.01200205301101093\n"
     ]
    }
   ],
   "source": [
    "# To convert prediction of SalePrice into the actual value, we take exponential value\n",
    "print(np.expm1(pred))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
