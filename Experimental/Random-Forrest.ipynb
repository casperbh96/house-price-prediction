{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREVIOUS WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Start Python Imports\n",
    "import math, time, random, datetime\n",
    "from math import sqrt\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "import missingno\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Preprocessing\n",
    "from scipy.linalg import svd\n",
    "import sklearn\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from fancyimpute import IterativeImputer\n",
    "from scipy.stats import zscore\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Modelling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV, KFold, ShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "sample_submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1452.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>730.500000</td>\n",
       "      <td>56.897260</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>10516.828082</td>\n",
       "      <td>6.099315</td>\n",
       "      <td>5.575342</td>\n",
       "      <td>1971.267808</td>\n",
       "      <td>1984.865753</td>\n",
       "      <td>103.685262</td>\n",
       "      <td>443.639726</td>\n",
       "      <td>...</td>\n",
       "      <td>94.244521</td>\n",
       "      <td>46.660274</td>\n",
       "      <td>21.954110</td>\n",
       "      <td>3.409589</td>\n",
       "      <td>15.060959</td>\n",
       "      <td>2.758904</td>\n",
       "      <td>43.489041</td>\n",
       "      <td>6.321918</td>\n",
       "      <td>2007.815753</td>\n",
       "      <td>180921.195890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>421.610009</td>\n",
       "      <td>42.300571</td>\n",
       "      <td>24.284752</td>\n",
       "      <td>9981.264932</td>\n",
       "      <td>1.382997</td>\n",
       "      <td>1.112799</td>\n",
       "      <td>30.202904</td>\n",
       "      <td>20.645407</td>\n",
       "      <td>181.066207</td>\n",
       "      <td>456.098091</td>\n",
       "      <td>...</td>\n",
       "      <td>125.338794</td>\n",
       "      <td>66.256028</td>\n",
       "      <td>61.119149</td>\n",
       "      <td>29.317331</td>\n",
       "      <td>55.757415</td>\n",
       "      <td>40.177307</td>\n",
       "      <td>496.123024</td>\n",
       "      <td>2.703626</td>\n",
       "      <td>1.328095</td>\n",
       "      <td>79442.502883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "      <td>34900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>365.750000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>7553.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>1967.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>129975.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>730.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>9478.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>383.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>163000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1095.250000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>11601.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>712.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>214000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>5644.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>857.000000</td>\n",
       "      <td>547.000000</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>15500.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>755000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id   MSSubClass  LotFrontage        LotArea  OverallQual  \\\n",
       "count  1460.000000  1460.000000  1201.000000    1460.000000  1460.000000   \n",
       "mean    730.500000    56.897260    70.049958   10516.828082     6.099315   \n",
       "std     421.610009    42.300571    24.284752    9981.264932     1.382997   \n",
       "min       1.000000    20.000000    21.000000    1300.000000     1.000000   \n",
       "25%     365.750000    20.000000    59.000000    7553.500000     5.000000   \n",
       "50%     730.500000    50.000000    69.000000    9478.500000     6.000000   \n",
       "75%    1095.250000    70.000000    80.000000   11601.500000     7.000000   \n",
       "max    1460.000000   190.000000   313.000000  215245.000000    10.000000   \n",
       "\n",
       "       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  ...  \\\n",
       "count  1460.000000  1460.000000   1460.000000  1452.000000  1460.000000  ...   \n",
       "mean      5.575342  1971.267808   1984.865753   103.685262   443.639726  ...   \n",
       "std       1.112799    30.202904     20.645407   181.066207   456.098091  ...   \n",
       "min       1.000000  1872.000000   1950.000000     0.000000     0.000000  ...   \n",
       "25%       5.000000  1954.000000   1967.000000     0.000000     0.000000  ...   \n",
       "50%       5.000000  1973.000000   1994.000000     0.000000   383.500000  ...   \n",
       "75%       6.000000  2000.000000   2004.000000   166.000000   712.250000  ...   \n",
       "max       9.000000  2010.000000   2010.000000  1600.000000  5644.000000  ...   \n",
       "\n",
       "        WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  ScreenPorch  \\\n",
       "count  1460.000000  1460.000000    1460.000000  1460.000000  1460.000000   \n",
       "mean     94.244521    46.660274      21.954110     3.409589    15.060959   \n",
       "std     125.338794    66.256028      61.119149    29.317331    55.757415   \n",
       "min       0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "50%       0.000000    25.000000       0.000000     0.000000     0.000000   \n",
       "75%     168.000000    68.000000       0.000000     0.000000     0.000000   \n",
       "max     857.000000   547.000000     552.000000   508.000000   480.000000   \n",
       "\n",
       "          PoolArea       MiscVal       MoSold       YrSold      SalePrice  \n",
       "count  1460.000000   1460.000000  1460.000000  1460.000000    1460.000000  \n",
       "mean      2.758904     43.489041     6.321918  2007.815753  180921.195890  \n",
       "std      40.177307    496.123024     2.703626     1.328095   79442.502883  \n",
       "min       0.000000      0.000000     1.000000  2006.000000   34900.000000  \n",
       "25%       0.000000      0.000000     5.000000  2007.000000  129975.000000  \n",
       "50%       0.000000      0.000000     6.000000  2008.000000  163000.000000  \n",
       "75%       0.000000      0.000000     8.000000  2009.000000  214000.000000  \n",
       "max     738.000000  15500.000000    12.000000  2010.000000  755000.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_ii(df):\n",
    "    df_filled_ii = pd.DataFrame(IterativeImputer().fit_transform(df.values))\n",
    "    df_filled_ii.columns = df.columns\n",
    "    df_filled_ii.index = df.index\n",
    "\n",
    "    return df_filled_ii\n",
    "\n",
    "# This function removes all observations that are more than\n",
    "# three standard deviations away from the mean\n",
    "def remove_outliers(df):\n",
    "    '''\n",
    "    numeric_features = train.select_dtypes(include=[np.number])\n",
    "    print(len(numeric_features.columns))\n",
    "    print(numeric_features.columns)\n",
    "    fig, axes = plt.subplots(ncols=5, nrows=8, figsize=(16, 40))\n",
    "    axes = np.ravel(axes)\n",
    "\n",
    "    col_name = ['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual',\n",
    "           'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n",
    "           'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n",
    "           'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
    "           'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n",
    "           'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n",
    "           'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n",
    "           'MiscVal', 'MoSold', 'YrSold', 'SalePrice']\n",
    "    for i, c in zip(range(38), col_name):\n",
    "        train.plot.scatter(ax=axes[i], x=c, y='SalePrice', sharey=True, colorbar=False, c='r')\n",
    "    '''\n",
    "    df = df.drop(df[(df['LotArea']>100000)].index)\n",
    "    df = df.drop(df[(df['BsmtFinSF1']>4000)].index)\n",
    "    df = df.drop(df[(df['TotalBsmtSF']>4000)].index)\n",
    "    df = df.drop(df[(df['1stFlrSF']>4000)].index)\n",
    "    df = df.drop(df[(df['GrLivArea']>4000)].index)\n",
    "    \n",
    "    return df\n",
    "        \n",
    "def data_engineering(train, test):\n",
    "    # Make train and test equal have the same shape\n",
    "    train = train.drop(train.index[0])\n",
    "    \n",
    "    # Concatenate all of data\n",
    "    cc_data = pd.concat([train, test], sort=True)\n",
    "    cc_data = cc_data.drop(['Id', 'SalePrice','Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis=1)\n",
    "    \n",
    "    # Get the SalePrice as the natural logarithm\n",
    "    train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "    y = train['SalePrice']\n",
    "    \n",
    "    # Remove outliers from data\n",
    "    cc_data = remove_outliers(cc_data)\n",
    "    \n",
    "    # One-Hot encode all data\n",
    "    cc_data = pd.get_dummies(cc_data, prefix_sep='_')\n",
    "    \n",
    "    # Impute all data, using IterativeImputer\n",
    "    cc_data = fill_ii(cc_data)\n",
    "    \n",
    "    # Slice data, start to middle and middle to end\n",
    "    X_train = cc_data[:train.shape[0]]\n",
    "    X_test = cc_data[train.shape[0]:]\n",
    "    \n",
    "    return X_train,X_test,y\n",
    "\n",
    "# X is dataframe, y is output, m is how many features you want selected\n",
    "# returns array of highest scoring features\n",
    "def feature_selection(X, y, m):\n",
    "    # Data is standardized here, minus mean and divided by standard deviation\n",
    "    # The correlation between each regressor and the target is computed\n",
    "    # It is converted to an F score then to a p-value, which is returned\n",
    "    f_regression = lambda X,y : sklearn.feature_selection.f_regression(X,y,center=False)\n",
    "\n",
    "    # removes all but the  highest scoring features\n",
    "    featureSelector = SelectKBest(score_func=f_regression,k=m)\n",
    "    featureSelector.fit(X,y)\n",
    "    high_score_arr = [X.columns[1+zero_based_index] for zero_based_index in list(featureSelector.get_support(indices=True))]\n",
    "    \n",
    "    return high_score_arr\n",
    "\n",
    "df_train,df_test,y = data_engineering(train,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1/5 <-- Current outer fold\n",
      "Optimal parameters: {'max_depth': [None], 'n_estimators': [1000]}\n",
      "Mean Test Error: 0.02952111107007361\n",
      "Mean Training error: 0.02912009854388337\n",
      "\n",
      "2/5 <-- Current outer fold\n",
      "Optimal parameters: {'max_depth': [None], 'n_estimators': [600]}\n",
      "Mean Test Error: 0.029267328640661\n",
      "Mean Training error: 0.029963228896132493\n",
      "\n",
      "3/5 <-- Current outer fold\n",
      "Optimal parameters: {'max_depth': [None], 'n_estimators': [800]}\n",
      "Mean Test Error: 0.029727113662829967\n",
      "Mean Training error: 0.029673495837410597\n",
      "\n",
      "4/5 <-- Current outer fold\n",
      "Optimal parameters: {'max_depth': [None], 'n_estimators': [1000]}\n",
      "Mean Test Error: 0.02949937464158821\n",
      "Mean Training error: 0.029095922233418246\n",
      "\n",
      "5/5 <-- Current outer fold\n",
      "Optimal parameters: {'max_depth': [None], 'n_estimators': [50]}\n",
      "Mean Test Error: 0.0302780873974049\n",
      "Mean Training error: 0.02963958322819895\n",
      "\n",
      "Generalization Error as RMSLE: 0.1722167328760813\n",
      "Generalization Training Error as RMSLE: 0.17175117393429581\n"
     ]
    }
   ],
   "source": [
    "opt_params = []\n",
    "dic_values_to_array = lambda dic: {key: [dic[key]]  for key in dic}\n",
    "def random_forest_validation(X_train,y_train,X_test,params_grid,cv,validate = True, predict_train = False):\n",
    "    gs = GridSearchCV(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        param_grid=params_grid, cv=cv,#cv=ShuffleSplit(test_size=0.10, n_splits =1, random_state=0),\n",
    "        n_jobs=-1, scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    model = gs.fit(X_train,y_train)\n",
    "    \n",
    "    if not predict_train:\n",
    "        pred = model.predict(X_test)\n",
    "    else:\n",
    "        pred = model.predict(X_train)\n",
    "    \n",
    "    score = -model.best_score_\n",
    "    \n",
    "    if validate:\n",
    "        return dic_values_to_array(model.best_params_)\n",
    "    elif predict_train:\n",
    "        return score\n",
    "    else:\n",
    "        return pred,score\n",
    "\n",
    "def nested_cv(train_data,test_data,y,outer,inner):\n",
    "    '''\n",
    "    This function runs nested cross-validation, where the optimal parameters\n",
    "    for a random forest algorithm is searched for in the inner loop and\n",
    "    applied in the outer loop.\n",
    "    \n",
    "    train_data: your training data\n",
    "    test_data: your testing data\n",
    "    y: your output variable\n",
    "    outer: how many k-folds we split the data in the outer loop\n",
    "    inner: how many k-folds we split the data in the inner loop\n",
    "    '''\n",
    "    \n",
    "    # Define Cross-Validation with k-outer folds\n",
    "    CV = KFold(outer, shuffle=True)\n",
    "    \n",
    "    # Generalization Error array used for calculating\n",
    "    generalization_error = np.zeros(outer)\n",
    "    training_error = np.zeros(outer)\n",
    "    \n",
    "    # Split training data and output variable\n",
    "    for (i, (train_index, test_index)) in enumerate(CV.split(train_data,y)):\n",
    "        print('\\n{0}/{1} <-- Current outer fold'.format(i+1,outer))\n",
    "        \n",
    "        # Split data into training, output variable and test data\n",
    "        X_validation = train_data[:train_index.shape[0]]\n",
    "        y_validation = y[:train_index.shape[0]]\n",
    "        X_test = train_data[:test_index.shape[0]]\n",
    "        \n",
    "        # Use this if you don't have a test dataset\n",
    "        #X_test_outer = train_data[:test_index.shape[0]]\n",
    "        \n",
    "        # Define parameters for optimization\n",
    "        params_grid={\n",
    "            'max_depth': [3, None],\n",
    "            'n_estimators': (10, 20, 30, 50, 100, 200, 400, 600, 800, 1000)\n",
    "        }\n",
    "        \n",
    "        # Use inner data to find optimal parameters for RF\n",
    "        opt_params = random_forest_validation(X_validation,y_validation,X_test,params_grid,inner)\n",
    "        \n",
    "        # Run RF with optimal parameters\n",
    "        cv=ShuffleSplit(test_size=0.10, n_splits =1, random_state=0)\n",
    "        pred,generalization_error[i] = random_forest_validation(X_validation,y_validation,test_data,opt_params,cv,validate = False)\n",
    "        \n",
    "        print(\"Optimal parameters: {0}\".format(opt_params))\n",
    "        print(\"Mean Test Error: {0}\".format(generalization_error[i]))\n",
    "        \n",
    "        training_error[i] = random_forest_validation(train_data,y,test_data,opt_params,cv,validate = False, predict_train = True)\n",
    "        print(\"Mean Training error: {0}\".format(training_error[i]))\n",
    "        \n",
    "    print(\"\\nGeneralization Error as RMSLE: {0}\".format(sqrt(np.mean(generalization_error))))\n",
    "    print(\"Generalization Training Error as RMSLE: {0}\".format(sqrt(np.mean(training_error))))\n",
    "    \n",
    "nested_cv(df_train,df_test,y,5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def random_forest_prediction(X_train,X_test,y_real):\n",
    "    gs = GridSearchCV(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        param_grid={\n",
    "            'max_depth': [3, None],\n",
    "            'n_estimators': (10, 30, 50, 100, 200, 400, 600, 800, 1000),\n",
    "            'max_features': (2,4,6)\n",
    "        }, cv=10, n_jobs=-1, scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    model = gs.fit(X_train,y_real)\n",
    "    pred = model.predict(X_test)\n",
    "    score = sqrt(-model.best_score_)\n",
    "    \n",
    "    # return all predictions and mean of all cross validated scores\n",
    "    return pred, score, model\n",
    "\n",
    "df_train,df_test,y = data_engineering(train,test)\n",
    "#selected_features = feature_selection(df_train, y, 50)\n",
    "\n",
    "#pred,score, model = random_forest_prediction(df_train, df_test, y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
