{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "t6W7PQysnFNx",
    "outputId": "4357a8cb-3582-4165-c771-e8156bcb26fd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c73a6e5650ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_engineering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-c73a6e5650ab>\u001b[0m in \u001b[0;36mdata_engineering\u001b[1;34m(train, test)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mcc_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcc_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix_sep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mcc_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfill_ii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcc_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcc_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-c73a6e5650ab>\u001b[0m in \u001b[0;36mfill_ii\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfill_ii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mdf_filled_ii\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIterativeImputer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mdf_filled_ii\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mdf_filled_ii\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\fancyimpute\\iterative_imputer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    936\u001b[0m                 Xt, predictor = self._impute_one_feature(\n\u001b[0;32m    937\u001b[0m                     \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_missing_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeat_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneighbor_feat_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 938\u001b[1;33m                     predictor=None, fit_mode=True)\n\u001b[0m\u001b[0;32m    939\u001b[0m                 predictor_triplet = ImputerTriplet(feat_idx,\n\u001b[0;32m    940\u001b[0m                                                    \u001b[0mneighbor_feat_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\fancyimpute\\iterative_imputer.py\u001b[0m in \u001b[0;36m_impute_one_feature\u001b[1;34m(self, X_filled, mask_missing_values, feat_idx, neighbor_feat_idx, predictor, fit_mode)\u001b[0m\n\u001b[0;32m    674\u001b[0m             y_train = safe_indexing(X_filled[:, feat_idx],\n\u001b[0;32m    675\u001b[0m                                     ~missing_row_mask)\n\u001b[1;32m--> 676\u001b[1;33m             \u001b[0mpredictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m         \u001b[1;31m# get posterior samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1146\u001b[0m                                   \u001b[0mgcv_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgcv_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m                                   store_cv_values=self.store_cv_values)\n\u001b[1;32m-> 1148\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstore_cv_values\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1056\u001b[0m         \u001b[0mcentered_kernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1058\u001b[1;33m         \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQT_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_pre_compute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcentered_kernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1059\u001b[0m         \u001b[0mn_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m         \u001b[0mcv_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malphas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py\u001b[0m in \u001b[0;36m_pre_compute_svd\u001b[1;34m(self, X, y, centered_kernel)\u001b[0m\n\u001b[0;32m    968\u001b[0m         \u001b[1;31m# to emulate fit_intercept=True situation, add a column on ones\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m         \u001b[1;31m# Note that by centering, the other columns are orthogonal to that one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 970\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    971\u001b[0m         \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m         \u001b[0mUT_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mU\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp_svd.py\u001b[0m in \u001b[0;36msvd\u001b[1;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;31m# perform decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     u, s, v, info = gesXd(a1, compute_uv=compute_uv, lwork=lwork,\n\u001b[1;32m--> 129\u001b[1;33m                           full_matrices=full_matrices, overwrite_a=overwrite_a)\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fancyimpute import IterativeImputer\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold, ParameterGrid, ParameterSampler\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "train = pd.read_csv('https://raw.githubusercontent.com/casperbh96/house-price-prediction/master/Experimental/data/train.csv')\n",
    "test = pd.read_csv('https://raw.githubusercontent.com/casperbh96/house-price-prediction/master/Experimental/data/test.csv')\n",
    "\n",
    "def fill_ii(df):\n",
    "    df_filled_ii = pd.DataFrame(IterativeImputer().fit_transform(df.values))\n",
    "    df_filled_ii.columns = df.columns\n",
    "    df_filled_ii.index = df.index\n",
    "\n",
    "    return df_filled_ii\n",
    "\n",
    "def data_engineering(train, test):\n",
    "    train = train.drop(train.index[0])\n",
    "    \n",
    "    cc_data = pd.concat([train, test], sort=True)\n",
    "    cc_data = cc_data.drop(['Id', 'SalePrice','Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis=1)\n",
    "    \n",
    "    train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "    y = train['SalePrice']\n",
    "    \n",
    "    cc_data = pd.get_dummies(cc_data, prefix_sep='_')\n",
    "    \n",
    "    cc_data = fill_ii(cc_data)\n",
    "    \n",
    "    X_train = cc_data[:train.shape[0]]\n",
    "    X_test = cc_data[train.shape[0]:]\n",
    "    \n",
    "    return X_train,X_test,y\n",
    "\n",
    "X,X_test,y = data_engineering(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l3gXfOMgoODT"
   },
   "outputs": [],
   "source": [
    "class NestedCV():\n",
    "    '''A general class to handle nested cross-validation for any estimator that\n",
    "    implements the scikit-learn estimator interface.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : estimator\n",
    "        The estimator implements scikit-learn estimator interface.\n",
    "\n",
    "    params_grid : dict\n",
    "        The dict contains hyperparameters for model.\n",
    "\n",
    "    outer_kfolds : int\n",
    "        Number of outer K-partitions in KFold\n",
    "\n",
    "    inner_kfolds : int\n",
    "        Number of inner K-partitions in KFold\n",
    "\n",
    "    cv_options: dict, default = {}\n",
    "        Nested CV Options, check docs for details.\n",
    "\n",
    "        metric : callable from sklearn.metrics, default = mean_squared_error\n",
    "            A scoring metric used to score each model\n",
    "\n",
    "        metric_score_indicator_lower : boolean, default = True\n",
    "            Choose whether lowe score is better for the metric calculation or hight score is better, `True` means lower score is better.\n",
    "\n",
    "        sqrt_of_score : boolean, default = False\n",
    "            Whether or not if the square root should be taken of score\n",
    "\n",
    "        randomized_search : boolean, default = True\n",
    "            Whether to use gridsearch or randomizedsearch from sklearn\n",
    "\n",
    "        randomized_search_iter : int, default = 10\n",
    "            Number of iterations for randomized search\n",
    "\n",
    "        recursive_feature_elimination : boolean, default = False\n",
    "            Whether to do feature elimination\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model, params_grid, outer_kfolds, inner_kfolds, cv_options={}):\n",
    "        self.model = model\n",
    "        self.params_grid = params_grid\n",
    "        self.outer_kfolds = outer_kfolds\n",
    "        self.inner_kfolds = inner_kfolds\n",
    "        self.metric = cv_options.get('metric', mean_squared_error)\n",
    "        self.metric_score_indicator_lower = cv_options.get(\n",
    "            'metric_score_indicator_lower', True)\n",
    "        self.sqrt_of_score = cv_options.get('sqrt_of_score', False)\n",
    "        self.randomized_search = cv_options.get('randomized_search', True)\n",
    "        self.randomized_search_iter = cv_options.get(\n",
    "            'randomized_search_iter', 10)\n",
    "        self.recursive_feature_elimination = cv_options.get(\n",
    "            'recursive_feature_elimination', False)\n",
    "        self.outer_scores = []\n",
    "        self.best_params = {}\n",
    "        self.best_inner_score_list = []\n",
    "        self.variance = []\n",
    "\n",
    "    # to check if use sqrt_of_score and handle the different cases\n",
    "    def _transform_score_format(self, scoreValue):\n",
    "        if self.sqrt_of_score:\n",
    "            return np.sqrt(scoreValue)\n",
    "        return scoreValue\n",
    "\n",
    "    # to convert array of dict to dict with array values, so it can be used as params for parameter tuning\n",
    "    def _score_to_best_params(self, best_inner_params_list):\n",
    "        params_dict = {}\n",
    "        for best_inner_params in best_inner_params_list:\n",
    "            for key, value in best_inner_params.items():\n",
    "                if key in params_dict:\n",
    "                    if value not in params_dict[key]:\n",
    "                        params_dict[key].append(value)\n",
    "                else:\n",
    "                    params_dict[key] = [value]\n",
    "        return params_dict\n",
    "\n",
    "    # a method to handle  recursive feature elimination\n",
    "    def _fit_recursive_feature_elimination(self, best_inner_params, X_train_outer, y_train_outer, X_test_outer):\n",
    "        print('\\nRunning recursive feature elimination for outer loop... (SLOW)')\n",
    "        # K-fold (inner_kfolds) recursive feature elimination\n",
    "        rfe = RFECV(estimator=self.model, min_features_to_select=20,\n",
    "                    scoring='neg_mean_squared_error', cv=self.inner_kfolds, n_jobs=-1)\n",
    "        rfe.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "        # Assign selected features to data\n",
    "        print('Best number of features was: {0}'.format(rfe.n_features_))\n",
    "        X_train_outer_rfe = rfe.transform(X_train_outer)\n",
    "        X_test_outer_rfe = rfe.transform(X_test_outer)\n",
    "\n",
    "        # Train model with best inner parameters on the outer split\n",
    "        self.model.set_params(**best_inner_params)\n",
    "        self.model.fit(X_train_outer_rfe, y_train_outer)\n",
    "        return self.model.predict(X_test_outer_rfe)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''A method to fit nested cross-validation \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas dataframe (rows, columns)\n",
    "            Training dataframe, where rows is total number of observations and columns\n",
    "            is total number of features\n",
    "    \n",
    "        y : pandas dataframe\n",
    "            Output dataframe, also called output variable. y is what you want to predict.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        It will not return directly the values, but it's accessable from the class object it self.\n",
    "        You should be able to access:\n",
    "        \n",
    "        variance\n",
    "            Model variance by numpy.var()\n",
    "            \n",
    "        outer_scores \n",
    "            Outer score List.\n",
    "            \n",
    "        best_inner_score_list \n",
    "            Best inner scores for each outer loop\n",
    "            \n",
    "        best_params \n",
    "            All best params from each inner loop cumulated in a dict\n",
    "            \n",
    "        best_inner_params_list \n",
    "            Best inner params for each outer loop as an array of dictionaries\n",
    "        '''\n",
    "    \n",
    "        print('\\n{0} <-- Running this model now'.format(type(self.model).__name__))\n",
    "        outer_cv = KFold(n_splits=self.outer_kfolds, shuffle=True)\n",
    "        inner_cv = KFold(n_splits=self.inner_kfolds, shuffle=True)\n",
    "        model = self.model\n",
    "    \n",
    "        outer_scores = []\n",
    "        variance = []\n",
    "        best_inner_params_list = []  # Change both to by one thing out of key-value pair\n",
    "        best_inner_score_list = []\n",
    "    \n",
    "        # Split X and y into K-partitions to Outer CV\n",
    "        for (i, (train_index, test_index)) in enumerate(outer_cv.split(X, y)):\n",
    "            print('\\n{0}/{1} <-- Current outer fold'.format(i+1, self.outer_kfolds))\n",
    "            X_train_outer, X_test_outer = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train_outer, y_test_outer = y.iloc[train_index], y.iloc[test_index]\n",
    "            best_inner_params = {}\n",
    "            best_inner_score = None\n",
    "    \n",
    "            # Split X_train_outer and y_train_outer into K-partitions to be inner CV\n",
    "            for (j, (train_index_inner, test_index_inner)) in enumerate(inner_cv.split(X_train_outer, y_train_outer)):\n",
    "                print('\\n\\t{0}/{1} <-- Current inner fold'.format(j+1, self.inner_kfolds))\n",
    "                X_train_inner, X_test_inner = X_train_outer.iloc[\n",
    "                    train_index_inner], X_train_outer.iloc[test_index_inner]\n",
    "                y_train_inner, y_test_inner = y_train_outer.iloc[\n",
    "                    train_index_inner], y_train_outer.iloc[test_index_inner]\n",
    "    \n",
    "                # Run either RandomizedSearch or GridSearch for input parameters\n",
    "                for param_dict in ParameterSampler(param_distributions=self.params_grid, \n",
    "                                                   n_iter=self.randomized_search_iter) if (self.randomized_search) else (\n",
    "                                                           ParameterGrid(param_grid=self.params_grid)):\n",
    "                    # Set parameters, train model on inner split, predict results.\n",
    "                    if(type(self.model).__name__ == 'KerasRegressor'):\n",
    "                        with tf.device('/gpu:0'):\n",
    "                          model.set_params(**param_dict)\n",
    "                          model.fit(X_train_inner, y_train_inner)\n",
    "                    else:\n",
    "                      model.set_params(**param_dict)\n",
    "                      model.fit(X_train_inner, y_train_inner)\n",
    "                    \n",
    "                    \n",
    "                    inner_pred = model.predict(X_test_inner)\n",
    "                    inner_grid_score = self.metric(y_test_inner, inner_pred)\n",
    "                    current_inner_score_value = best_inner_score\n",
    "                    # Find best score and corresponding best grid\n",
    "                    if(best_inner_score is not None):\n",
    "                        if(self.metric_score_indicator_lower and best_inner_score > inner_grid_score):\n",
    "                            best_inner_score = self._transform_score_format(inner_grid_score)\n",
    "                            \n",
    "                        elif (not self.metric_score_indicator_lower and best_inner_score < inner_grid_score):\n",
    "                            best_inner_score = self._transform_score_format(inner_grid_score)\n",
    "                    else:\n",
    "                        best_inner_score = self._transform_score_format(inner_grid_score)\n",
    "                        current_inner_score_value = best_inner_score+1  # first time random thing\n",
    "                        \n",
    "                    # Update best_inner_grid once rather than calling it under each if statement\n",
    "                    if(current_inner_score_value is not None and current_inner_score_value != best_inner_score):\n",
    "                        best_inner_params = param_dict\n",
    "    \n",
    "            best_inner_params_list.append(best_inner_params)\n",
    "            best_inner_score_list.append(best_inner_score)\n",
    "    \n",
    "            if self.recursive_feature_elimination:\n",
    "                pred = self._fit_recursive_feature_elimination(\n",
    "                    best_inner_params, X_train_outer, y_train_outer, X_test_outer)\n",
    "            else:\n",
    "                # Train model with best inner parameters on the outer split\n",
    "                model.set_params(**best_inner_params)\n",
    "                model.fit(X_train_outer, y_train_outer)\n",
    "                pred = model.predict(X_test_outer)\n",
    "    \n",
    "            outer_scores.append(self._transform_score_format(\n",
    "                self.metric(y_test_outer, pred)))\n",
    "    \n",
    "            # Append variance\n",
    "            variance.append(np.var(pred, ddof=1))\n",
    "    \n",
    "            print('\\nResults for outer fold:\\nBest inner parameters was: {0}'.format(\n",
    "                best_inner_params_list[i]))\n",
    "            print('Outer score: {0}'.format(outer_scores[i]))\n",
    "            print('Inner score: {0}'.format(best_inner_score_list[i]))\n",
    "    \n",
    "        self.variance = variance\n",
    "        self.outer_scores = outer_scores\n",
    "        self.best_inner_score_list = best_inner_score_list\n",
    "        self.best_params = self._score_to_best_params(best_inner_params_list)\n",
    "        self.best_inner_params_list = best_inner_params_list\n",
    "\n",
    "    # Method to show score vs variance chart. You can run it only after fitting the model.\n",
    "    def score_vs_variance_plot(self):\n",
    "        # Plot score vs variance\n",
    "        plt.figure()\n",
    "        plt.subplot(211)\n",
    "\n",
    "        variance_plot, = plt.plot(self.variance, color='b')\n",
    "        score_plot, = plt.plot(self.outer_scores, color='r')\n",
    "\n",
    "        plt.legend([variance_plot, score_plot],\n",
    "                   [\"Variance\", \"Score\"],\n",
    "                   bbox_to_anchor=(0, .4, .5, 0))\n",
    "\n",
    "        plt.title(\"{0}: Score VS Variance\".format(type(self.model).__name__),\n",
    "                  x=.5, y=1.1, fontsize=\"15\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NestedCV parameters\n",
    "NUM_TRIALS = 50\n",
    "outerFolds = 5\n",
    "innerFolds = 5\n",
    "\n",
    "models_to_run = [xgb.XGBRegressor()]\n",
    "models_param_grid = [\n",
    "                    { # 3rd param grid, corresponding to XGBRegressor\n",
    "                            'learning_rate': [0.05],\n",
    "                            'colsample_bytree': np.linspace(0.3, 0.5),\n",
    "                            'n_estimators': [100,200,300,400,500,600,700,800,900,1000],\n",
    "                            'reg_alpha' : (1,1.2),\n",
    "                            'reg_lambda' : (1,1.2,1.4)\n",
    "                    }\n",
    "                    ]\n",
    "XGB_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "0pKVdTkjogUk",
    "outputId": "cd9796fc-a54f-4f49-e747-fe3f1f48945f"
   },
   "outputs": [],
   "source": [
    "for trial in range(NUM_TRIALS):\n",
    "    print('Running {0} / {1}'.format(trial,NUM_TRIALS))\n",
    "    for i,model in enumerate(models_to_run):\n",
    "        nested_CV_search = NestedCV(model=model, params_grid=models_param_grid[i], outer_kfolds=outerFolds, inner_kfolds=innerFolds, \n",
    "                          cv_options={'sqrt_of_score':True, 'randomized_search_iter':30})\n",
    "        nested_CV_search.fit(X=X,y=y)\n",
    "        model_param_grid = nested_CV_search.best_params\n",
    "        print('\\nCumulated best parameter grids was:\\n{0}'.format(model_param_grid))\n",
    "        \n",
    "        gscv = GridSearchCV(estimator=model,param_grid=model_param_grid,scoring='neg_mean_squared_error',cv=5, n_jobs=-1)\n",
    "        gscv.fit(X,y)\n",
    "        \n",
    "        print('\\nFitting with optimal parameters:\\n{0}'.format(gscv.best_params_))\n",
    "        gscv.predict(X_test)\n",
    "        score = np.sqrt(-gscv.best_score_)\n",
    "        \n",
    "        if(type(model).__name__ == 'KerasRegressor'):\n",
    "            NN_scores.append(score)\n",
    "        elif(type(model).__name__ == 'RandomForestRegressor'):\n",
    "            RF_scores.append(score)\n",
    "        elif(type(model).__name__ == 'XGBRegressor'):\n",
    "            XGB_scores.append(score)\n",
    "        \n",
    "        print('\\nFinal score for {0} was {1}'.format(type(model).__name__,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "xgb, = plt.plot(XGB_scores, color='r')\n",
    "\n",
    "plt.legend([xgb],\n",
    "           [\"XGBoost\"],\n",
    "           bbox_to_anchor=(0, .4, .5, 0))\n",
    "\n",
    "plt.title('Test scores as RMSLE with hyperparameter optimization',\n",
    "          x=.5, y=1.1, fontsize=\"15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "54lkC72t-coi"
   },
   "outputs": [],
   "source": [
    "print(XGB_scores)\n",
    "print('XG boost generalization score: {0}'.format(np.mean(XGB_scores)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "House_Prices.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
