{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREVIOUS WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Python Imports\n",
    "import math, time, random, datetime\n",
    "from math import sqrt\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "import missingno\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Preprocessing\n",
    "from scipy.linalg import svd\n",
    "import sklearn\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from fancyimpute import IterativeImputer\n",
    "from scipy.stats import zscore\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Modelling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "sample_submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_or_nan_values(df):\n",
    "    fill_with = 0\n",
    "    \n",
    "    # Get the most common element by using size(),\n",
    "    # which returns the element and how common it is\n",
    "    for column in df:\n",
    "        \n",
    "        # Check if the column is an object, float64 or int64\n",
    "        is_it_float = (df[column].dtype == np.float64)\n",
    "        is_it_int = (df[column].dtype == np.int64)\n",
    "        \n",
    "        # If it is an object,\n",
    "        # find the most common element and fill missing and NaN values\n",
    "        if(not is_it_float and not is_it_int):\n",
    "            fill_with = df[column].mode().item()\n",
    "                    \n",
    "        # If it is either a float64 or int64,\n",
    "        # then calculate the mean and fill missing and NaN values\n",
    "        else:\n",
    "            if is_it_float:\n",
    "                fill_with = np.nanmean(df[column], dtype=np.float64)\n",
    "            if is_it_int:\n",
    "                fill_with = np.nanmean(df[column], dtype=np.int64)\n",
    "        \n",
    "        # Fill the values in our dataset\n",
    "        df[column] = df[column].fillna(fill_with)\n",
    "        fill_with = 0\n",
    "\n",
    "def fill_ii(df):\n",
    "    df_filled_ii = pd.DataFrame(IterativeImputer().fit_transform(df.as_matrix()))\n",
    "    df_filled_ii.columns = df.columns\n",
    "    df_filled_ii.index = df.index\n",
    "\n",
    "    return df_filled_ii\n",
    "\n",
    "def pca(df):\n",
    "    pca = PCA(.95)\n",
    "    df = pca.fit_transform(df)\n",
    "    return df\n",
    "\n",
    "# This function removes all observations that are more than\n",
    "# three standard deviations away from the mean\n",
    "def remove_outliers(df):\n",
    "    '''\n",
    "    numeric_features = train.select_dtypes(include=[np.number])\n",
    "    print(len(numeric_features.columns))\n",
    "    print(numeric_features.columns)\n",
    "    fig, axes = plt.subplots(ncols=5, nrows=8, figsize=(16, 40))\n",
    "    axes = np.ravel(axes)\n",
    "\n",
    "    col_name = ['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual',\n",
    "           'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n",
    "           'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n",
    "           'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
    "           'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n",
    "           'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n",
    "           'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n",
    "           'MiscVal', 'MoSold', 'YrSold', 'SalePrice']\n",
    "    for i, c in zip(range(38), col_name):\n",
    "        train.plot.scatter(ax=axes[i], x=c, y='SalePrice', sharey=True, colorbar=False, c='r')\n",
    "    '''\n",
    "    df = df.drop(df[(df['LotArea']>100000)].index)\n",
    "    df = df.drop(df[(df['BsmtFinSF1']>4000)].index)\n",
    "    df = df.drop(df[(df['TotalBsmtSF']>4000)].index)\n",
    "    df = df.drop(df[(df['1stFlrSF']>4000)].index)\n",
    "    df = df.drop(df[(df['GrLivArea']>4000)].index)\n",
    "    \n",
    "    return df\n",
    "        \n",
    "def data_engineering(train, test):\n",
    "    # Concatenate all of data\n",
    "    cc_data = pd.concat([train, test])\n",
    "    cc_data = cc_data.drop(['Id', 'SalePrice','Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis=1)\n",
    "    \n",
    "    # Get the SalePrice as the natural logarithm\n",
    "    train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "    y = train['SalePrice']\n",
    "    \n",
    "    # Remove outliers from data\n",
    "    #cc_data = remove_outliers(cc_data)\n",
    "    \n",
    "    # One-Hot encode all data\n",
    "    cc_data = pd.get_dummies(cc_data, prefix_sep='_')\n",
    "    \n",
    "    # Impute all data, using IterativeImputer\n",
    "    cc_data = fill_ii(cc_data)\n",
    "    \n",
    "    # Slice data, train.shape[0] is the observations\n",
    "    # 1) from start to middle of observations\n",
    "    # 2) from middle of observations to end\n",
    "    X_train = cc_data[:train.shape[0]]\n",
    "    X_test = cc_data[train.shape[0]:]\n",
    "    \n",
    "    return X_train,X_test,y\n",
    "\n",
    "# X is dataframe, y is output, m is how many features you want selected\n",
    "# returns array of highest scoring features\n",
    "def feature_selection(X, y, m):\n",
    "    # Data is standardized here, minus mean and divided by standard deviation\n",
    "    # The correlation between each regressor and the target is computed\n",
    "    # It is converted to an F score then to a p-value, which is returned\n",
    "    f_regression = lambda X,y : sklearn.feature_selection.f_regression(X,y,center=False)\n",
    "\n",
    "    # removes all but the  highest scoring features\n",
    "    featureSelector = SelectKBest(score_func=f_regression,k=m)\n",
    "    featureSelector.fit(X,y)\n",
    "    high_score_arr = [X.columns[1+zero_based_index] for zero_based_index in list(featureSelector.get_support(indices=True))]\n",
    "    \n",
    "    return high_score_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_lvl_cv(X,y,outer,inner,algo_validation,params,search_function):\n",
    "    '''\n",
    "    This function runs nested cross-validation (or two-level cross-validation).\n",
    "    First it splits your data into 'outer' partitions, and on each outer\n",
    "    partition, it splits into 'inner' partitions. Inner and outer partitions\n",
    "    are some finite number, usually recommended to be either 5 or 10.\n",
    "    In the nested loop, it searches for the optimal parameters through a list\n",
    "    of parameters that you provide. It trains and tests 10 models, and the\n",
    "    model with the best lowest error value will be run on the outer \n",
    "    partition of the data. Using Mean Squared Error is recommended.\n",
    "    \n",
    "    This function only supports Pandas dataframes, and does not support\n",
    "    tensors for neural networks.\n",
    "    \n",
    "    Params for this function:\n",
    "    - X: All your data, but without the output variable y\n",
    "    - y: Your output variable\n",
    "    - outer: How many partitions df is split into, in the outer loop\n",
    "    - inner: How many partitions each partition from outer is split into\n",
    "    - algo: Custom function that runs your algorith, takes\n",
    "            X_train,y_train,X_test,y_test,optimal_param as input.\n",
    "            This function should return the MSE after running.\n",
    "    - params: Array of parameter values\n",
    "    - search_function: Your function for iterating over params, returns\n",
    "                       MSE and optimal parameter, the parameter with the\n",
    "                       lowest error from the array of params. This function\n",
    "                       should have X_train,y_train,X_test,y_test,params\n",
    "                       as input.\n",
    "    '''\n",
    "    CV = KFold(outer, shuffle=True)\n",
    "    for (i, (train_index, test_index)) in enumerate(CV.split(X,y)):\n",
    "        print('\\nCrossvalidation outer fold: {0}/{1}'.format(i+1,outer))\n",
    "        \n",
    "        X_train_outer = X[train_index,:]\n",
    "        y_train_outer = y[train_index]\n",
    "        X_test_outer = X[test_index,:]\n",
    "        y_test_outer = y[test_index]\n",
    "        \n",
    "        error = np.zeros([len(params),outer])\n",
    "        opt_params = np.zeros([len(params),outer])\n",
    "        \n",
    "        for (j, (train_index_inner, test_index_inner)) in enumerate(CV.split(X_train_outer,y_train_outer)):\n",
    "            print('\\nCrossvalidation inner fold: {0}/{1}'.format(j+1,inner))\n",
    "            \n",
    "            X_train_inner = X_train_outer[train_index_inner,:]\n",
    "            y_train_inner = y_train_outer[train_index_inner]\n",
    "            X_test_inner = X_train_outer[test_index_inner,:]\n",
    "            y_test_inner = y_train_outer[test_index_inner]\n",
    "            \n",
    "            error[:,j], opt_params[j] = search_function(X_train_inner,y_train_inner,X_test_inner,y_test_inner,params)\n",
    "        \n",
    "        current_lowest_error = 999999999\n",
    "        opt_index = 0\n",
    "        algo_generalization_error = []\n",
    "        error_measurement = []\n",
    "        \n",
    "        for i in range(0,inner):\n",
    "            algo_generalization_error.append((X_test_inner.shape[0]/X_train_outer.shape[0])*np.sum(mse[i]))\n",
    "        \n",
    "        for idx,i in enumerate(algo_generalization_error):\n",
    "            if i < current_lowest_error:\n",
    "                current_lowest_error = i\n",
    "                opt_index = idx\n",
    "        print(\"Lowest error: \",current_lowest_error)\n",
    "        print(\"Optimal parameter: \",opt_params[opt_index][i])\n",
    "        \n",
    "        error_measurement[i] = algo(X_train_outer,y_train_outer,X_test_outer,y_test_outer,opt_params[opt_index][i])\n",
    "        print(\"Ran your algorithm, this is the outcome: \", error_measurement[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caspe\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:72: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "C:\\Users\\caspe\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "def random_forest_prediction(X_train,X_test,y_real):\n",
    "    gs = GridSearchCV(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        param_grid={\n",
    "            'max_depth': [3, None],\n",
    "            'n_estimators': (10, 30, 50, 100, 200, 400, 600, 800, 1000)\n",
    "        }, cv=10, n_jobs=-1, scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    model = gs.fit(X_train,y_real)\n",
    "    pred = model.predict(X_test)\n",
    "    score = sqrt(-model.best_score_)\n",
    "    \n",
    "    # return all predictions and mean of all cross validated scores\n",
    "    return pred, score\n",
    "\n",
    "df_train,df_test,y = data_engineering(train,test)\n",
    "#selected_features = feature_selection(df_train, y, 50)\n",
    "\n",
    "#pred,score = random_forest_prediction(df_train, df_test, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[126297.0369045  152967.92929378 178457.4208113  ... 153067.78128057\n",
      " 114838.15811277 231787.84018998]\n",
      "0.1406194582622082\n"
     ]
    }
   ],
   "source": [
    "# To convert prediction of SalePrice into the actual value, we take exponential value\n",
    "print(np.expm1(pred))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
