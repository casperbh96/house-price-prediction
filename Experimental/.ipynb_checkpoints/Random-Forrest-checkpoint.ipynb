{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREVIOUS WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Python Imports\n",
    "import math, time, random, datetime\n",
    "from math import sqrt\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "import missingno\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Preprocessing\n",
    "from scipy.linalg import svd\n",
    "import sklearn\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from fancyimpute import IterativeImputer\n",
    "from scipy.stats import zscore\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Modelling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV, KFold, ShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "sample_submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_ii(df):\n",
    "    df_filled_ii = pd.DataFrame(IterativeImputer().fit_transform(df.values))\n",
    "    df_filled_ii.columns = df.columns\n",
    "    df_filled_ii.index = df.index\n",
    "\n",
    "    return df_filled_ii\n",
    "\n",
    "# This function removes all observations that are more than\n",
    "# three standard deviations away from the mean\n",
    "def remove_outliers(df):\n",
    "    '''\n",
    "    numeric_features = train.select_dtypes(include=[np.number])\n",
    "    print(len(numeric_features.columns))\n",
    "    print(numeric_features.columns)\n",
    "    fig, axes = plt.subplots(ncols=5, nrows=8, figsize=(16, 40))\n",
    "    axes = np.ravel(axes)\n",
    "\n",
    "    col_name = ['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual',\n",
    "           'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n",
    "           'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n",
    "           'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
    "           'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n",
    "           'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n",
    "           'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n",
    "           'MiscVal', 'MoSold', 'YrSold', 'SalePrice']\n",
    "    for i, c in zip(range(38), col_name):\n",
    "        train.plot.scatter(ax=axes[i], x=c, y='SalePrice', sharey=True, colorbar=False, c='r')\n",
    "    '''\n",
    "    df = df.drop(df[(df['LotArea']>100000)].index)\n",
    "    df = df.drop(df[(df['BsmtFinSF1']>4000)].index)\n",
    "    df = df.drop(df[(df['TotalBsmtSF']>4000)].index)\n",
    "    df = df.drop(df[(df['1stFlrSF']>4000)].index)\n",
    "    df = df.drop(df[(df['GrLivArea']>4000)].index)\n",
    "    \n",
    "    return df\n",
    "        \n",
    "def data_engineering(train, test):\n",
    "    # Make train and test equal have the same shape\n",
    "    train = train.drop(train.index[0])\n",
    "    \n",
    "    # Concatenate all of data\n",
    "    cc_data = pd.concat([train, test], sort=True)\n",
    "    cc_data = cc_data.drop(['Id', 'SalePrice','Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis=1)\n",
    "    \n",
    "    # Get the SalePrice as the natural logarithm\n",
    "    train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "    y = train['SalePrice']\n",
    "    \n",
    "    # Remove outliers from data\n",
    "    cc_data = remove_outliers(cc_data)\n",
    "    \n",
    "    # One-Hot encode all data\n",
    "    cc_data = pd.get_dummies(cc_data, prefix_sep='_')\n",
    "    \n",
    "    # Impute all data, using IterativeImputer\n",
    "    cc_data = fill_ii(cc_data)\n",
    "    \n",
    "    # Slice data, start to middle and middle to end\n",
    "    X_train = cc_data[:train.shape[0]]\n",
    "    X_test = cc_data[train.shape[0]:]\n",
    "    \n",
    "    return X_train,X_test,y\n",
    "\n",
    "# X is dataframe, y is output, m is how many features you want selected\n",
    "# returns array of highest scoring features\n",
    "def feature_selection(X, y, m):\n",
    "    # Data is standardized here, minus mean and divided by standard deviation\n",
    "    # The correlation between each regressor and the target is computed\n",
    "    # It is converted to an F score then to a p-value, which is returned\n",
    "    f_regression = lambda X,y : sklearn.feature_selection.f_regression(X,y,center=False)\n",
    "\n",
    "    # removes all but the  highest scoring features\n",
    "    featureSelector = SelectKBest(score_func=f_regression,k=m)\n",
    "    featureSelector.fit(X,y)\n",
    "    high_score_arr = [X.columns[1+zero_based_index] for zero_based_index in list(featureSelector.get_support(indices=True))]\n",
    "    \n",
    "    return high_score_arr\n",
    "\n",
    "df_train,df_test,y = data_engineering(train,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1/5 <-- Current outer fold\n",
      "Optimal parameters: {'max_depth': [None], 'n_estimators': [1000]}\n",
      "Mean Test Error: 0.02952111107007361\n",
      "Mean Training error: 0.02912009854388337\n",
      "\n",
      "2/5 <-- Current outer fold\n",
      "Optimal parameters: {'max_depth': [None], 'n_estimators': [600]}\n",
      "Mean Test Error: 0.029267328640661\n",
      "Mean Training error: 0.029963228896132493\n",
      "\n",
      "3/5 <-- Current outer fold\n",
      "Optimal parameters: {'max_depth': [None], 'n_estimators': [800]}\n",
      "Mean Test Error: 0.029727113662829967\n",
      "Mean Training error: 0.029673495837410597\n",
      "\n",
      "4/5 <-- Current outer fold\n",
      "Optimal parameters: {'max_depth': [None], 'n_estimators': [1000]}\n",
      "Mean Test Error: 0.02949937464158821\n",
      "Mean Training error: 0.029095922233418246\n",
      "\n",
      "5/5 <-- Current outer fold\n",
      "Optimal parameters: {'max_depth': [None], 'n_estimators': [50]}\n",
      "Mean Test Error: 0.0302780873974049\n",
      "Mean Training error: 0.02963958322819895\n",
      "\n",
      "Generalization Error as RMSLE: 0.1722167328760813\n",
      "Generalization Training Error as RMSLE: 0.17175117393429581\n"
     ]
    }
   ],
   "source": [
    "opt_params = []\n",
    "dic_values_to_array = lambda dic: {key: [dic[key]]  for key in dic}\n",
    "def random_forest_validation(X_train,y_train,X_test,params_grid,cv,validate = True, predict_train = False):\n",
    "    gs = GridSearchCV(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        param_grid=params_grid, cv=cv,#cv=ShuffleSplit(test_size=0.10, n_splits =1, random_state=0),\n",
    "        n_jobs=-1, scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    model = gs.fit(X_train,y_train)\n",
    "    \n",
    "    if not predict_train:\n",
    "        pred = model.predict(X_test)\n",
    "    else:\n",
    "        pred = model.predict(X_train)\n",
    "    \n",
    "    score = -model.best_score_\n",
    "    \n",
    "    if validate:\n",
    "        return dic_values_to_array(model.best_params_)\n",
    "    elif predict_train:\n",
    "        return score\n",
    "    else:\n",
    "        return pred,score\n",
    "\n",
    "def nested_cv(train_data,test_data,y,outer,inner):\n",
    "    '''\n",
    "    This function runs nested cross-validation, where the optimal parameters\n",
    "    for a random forest algorithm is searched for in the inner loop and\n",
    "    applied in the outer loop.\n",
    "    \n",
    "    train_data: your training data\n",
    "    test_data: your testing data\n",
    "    y: your output variable\n",
    "    outer: how many k-folds we split the data in the outer loop\n",
    "    inner: how many k-folds we split the data in the inner loop\n",
    "    '''\n",
    "    \n",
    "    # Define Cross-Validation with k-outer folds\n",
    "    CV = KFold(outer, shuffle=True)\n",
    "    \n",
    "    # Generalization Error array used for calculating\n",
    "    generalization_error = np.zeros(outer)\n",
    "    training_error = np.zeros(outer)\n",
    "    \n",
    "    # Split training data and output variable\n",
    "    for (i, (train_index, test_index)) in enumerate(CV.split(train_data,y)):\n",
    "        print('\\n{0}/{1} <-- Current outer fold'.format(i+1,outer))\n",
    "        \n",
    "        # Split data into training, output variable and test data\n",
    "        X_validation = train_data[:train_index.shape[0]]\n",
    "        y_validation = y[:train_index.shape[0]]\n",
    "        X_test = train_data[:test_index.shape[0]]\n",
    "        \n",
    "        # Use this if you don't have a test dataset\n",
    "        #X_test_outer = train_data[:test_index.shape[0]]\n",
    "        \n",
    "        # Define parameters for optimization\n",
    "        params_grid={\n",
    "            'max_depth': [3, None],\n",
    "            'n_estimators': (10, 20, 30, 50, 100, 200, 400, 600, 800, 1000)\n",
    "        }\n",
    "        \n",
    "        # Use inner data to find optimal parameters for RF\n",
    "        opt_params = random_forest_validation(X_validation,y_validation,X_test,params_grid,inner)\n",
    "        \n",
    "        # Run RF with optimal parameters\n",
    "        cv=ShuffleSplit(test_size=0.10, n_splits =1, random_state=0)\n",
    "        pred,generalization_error[i] = random_forest_validation(X_validation,y_validation,test_data,opt_params,cv,validate = False)\n",
    "        \n",
    "        print(\"Optimal parameters: {0}\".format(opt_params))\n",
    "        print(\"Mean Test Error: {0}\".format(generalization_error[i]))\n",
    "        \n",
    "        training_error[i] = random_forest_validation(train_data,y,test_data,opt_params,cv,validate = False, predict_train = True)\n",
    "        print(\"Mean Training error: {0}\".format(training_error[i]))\n",
    "        \n",
    "    print(\"\\nGeneralization Error as RMSLE: {0}\".format(sqrt(np.mean(generalization_error))))\n",
    "    print(\"Generalization Training Error as RMSLE: {0}\".format(sqrt(np.mean(training_error))))\n",
    "    \n",
    "nested_cv(df_train,df_test,y,5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def random_forest_prediction(X_train,X_test,y_real):\n",
    "    gs = GridSearchCV(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        param_grid={\n",
    "            'max_depth': [3, None],\n",
    "            'n_estimators': (10, 30, 50, 100, 200, 400, 600, 800, 1000),\n",
    "            'max_features': (2,4,6)\n",
    "        }, cv=10, n_jobs=-1, scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    model = gs.fit(X_train,y_real)\n",
    "    pred = model.predict(X_test)\n",
    "    score = sqrt(-model.best_score_)\n",
    "    \n",
    "    # return all predictions and mean of all cross validated scores\n",
    "    return pred, score, model\n",
    "\n",
    "df_train,df_test,y = data_engineering(train,test)\n",
    "#selected_features = feature_selection(df_train, y, 50)\n",
    "\n",
    "#pred,score, model = random_forest_prediction(df_train, df_test, y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
